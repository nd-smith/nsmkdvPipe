{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Maintenance Notebook\n",
    "\n",
    "This notebook performs regular maintenance tasks on Delta Lake tables in the Verisk Pipeline.\n",
    "\n",
    "## Maintenance Tasks\n",
    "\n",
    "1. **Retry Queue Cleanup** - Remove expired retry records older than retention period\n",
    "2. **Delta Table Optimization** - Compact small files for better query performance\n",
    "3. **VACUUM Operations** - Remove old file versions to reclaim storage\n",
    "4. **Health Reporting** - Generate statistics on table health and data quality\n",
    "\n",
    "## Scheduling\n",
    "\n",
    "This notebook should be scheduled to run daily during off-peak hours (e.g., 2:00 AM).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Update these values for your environment\n",
    "# =============================================================================\n",
    "\n",
    "# Lakehouse paths - Update these to match your Fabric lakehouse\n",
    "XACT_LAKEHOUSE_PATH = \"abfss://your-workspace@onelake.dfs.fabric.microsoft.com/your-xact-lakehouse.Lakehouse/Tables\"\n",
    "CLAIMX_LAKEHOUSE_PATH = \"abfss://your-workspace@onelake.dfs.fabric.microsoft.com/your-claimx-lakehouse.Lakehouse/Tables\"\n",
    "\n",
    "# Retention settings (in days)\n",
    "RETRY_RETENTION_DAYS = 30          # Delete retry records older than this\n",
    "EVENT_LOG_RETENTION_DAYS = 90      # Delete event log records older than this  \n",
    "VACUUM_RETENTION_HOURS = 168       # 7 days - Delta Lake minimum for time travel\n",
    "\n",
    "# Optimization settings\n",
    "TARGET_FILE_SIZE_MB = 128          # Target file size after compaction\n",
    "MIN_FILE_SIZE_MB = 10              # Files smaller than this trigger optimization\n",
    "SMALL_FILE_THRESHOLD = 10          # Optimize if more than this many small files\n",
    "\n",
    "# Safety settings\n",
    "DRY_RUN = False                    # Set to True to preview changes without applying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# TABLE DEFINITIONS\n# =============================================================================\n\n# XACT Pipeline Tables\nXACT_TABLES = {\n    \"xact_events\": {\n        \"path\": f\"{XACT_LAKEHOUSE_PATH}/xact_events\",\n        \"z_order_columns\": [\"event_date\", \"type\"],\n        \"partition_column\": \"event_date\",\n        \"description\": \"XACT webhook events from Kusto\"\n    },\n    \"xact_attachments\": {\n        \"path\": f\"{XACT_LAKEHOUSE_PATH}/xact_attachments\",\n        \"z_order_columns\": [\"event_date\", \"file_type\"],\n        \"partition_column\": \"event_date\",\n        \"description\": \"Successfully downloaded XACT attachments\"\n    },\n    \"xact_retry\": {\n        \"path\": f\"{XACT_LAKEHOUSE_PATH}/xact_retry\",\n        \"z_order_columns\": [\"status\", \"retry_count\", \"created_date\"],\n        \"partition_column\": \"created_date\",\n        \"cleanup\": True,\n        \"retention_days\": RETRY_RETENTION_DAYS,\n        \"description\": \"XACT attachment download retry queue\"\n    }\n}\n\n# ClaimX Pipeline Tables\nCLAIMX_TABLES = {\n    \"claimx_events\": {\n        \"path\": f\"{CLAIMX_LAKEHOUSE_PATH}/claimx_events\",\n        \"z_order_columns\": [\"event_date\", \"event_type\"],\n        \"partition_column\": \"event_date\",\n        \"description\": \"ClaimX events from Kusto\"\n    },\n    \"claimx_event_log\": {\n        \"path\": f\"{CLAIMX_LAKEHOUSE_PATH}/claimx_event_log\",\n        \"z_order_columns\": [\"processed_date\", \"status\"],\n        \"partition_column\": \"processed_date\",\n        \"cleanup\": True,\n        \"retention_days\": EVENT_LOG_RETENTION_DAYS,\n        \"cleanup_filter\": \"status IN ('success', 'failed_permanent')\",\n        \"description\": \"ClaimX API enrichment processing log\"\n    },\n    \"claimx_projects\": {\n        \"path\": f\"{CLAIMX_LAKEHOUSE_PATH}/claimx_projects\",\n        \"z_order_columns\": [\"project_id\"],\n        \"description\": \"ClaimX project entities\"\n    },\n    \"claimx_contacts\": {\n        \"path\": f\"{CLAIMX_LAKEHOUSE_PATH}/claimx_contacts\",\n        \"z_order_columns\": [\"project_id\", \"created_date\"],\n        \"partition_column\": \"created_date\",\n        \"description\": \"ClaimX contact entities\"\n    },\n    \"claimx_attachment_metadata\": {\n        \"path\": f\"{CLAIMX_LAKEHOUSE_PATH}/claimx_attachment_metadata\",\n        \"z_order_columns\": [\"project_id\", \"media_id\"],\n        \"description\": \"ClaimX media metadata from API\"\n    },\n    \"claimx_attachments\": {\n        \"path\": f\"{CLAIMX_LAKEHOUSE_PATH}/claimx_attachments\",\n        \"z_order_columns\": [\"created_date\", \"file_type\"],\n        \"partition_column\": \"created_date\",\n        \"description\": \"Successfully downloaded ClaimX attachments\"\n    },\n    \"claimx_retry\": {\n        \"path\": f\"{CLAIMX_LAKEHOUSE_PATH}/claimx_retry\",\n        \"z_order_columns\": [\"status\", \"retry_count\", \"created_date\"],\n        \"partition_column\": \"created_date\",\n        \"cleanup\": True,\n        \"retention_days\": RETRY_RETENTION_DAYS,\n        \"description\": \"ClaimX media download retry queue\"\n    },\n    \"claimx_tasks\": {\n        \"path\": f\"{CLAIMX_LAKEHOUSE_PATH}/claimx_tasks\",\n        \"z_order_columns\": [\"project_id\", \"assignment_id\"],\n        \"description\": \"ClaimX task assignments\"\n    },\n    \"claimx_task_templates\": {\n        \"path\": f\"{CLAIMX_LAKEHOUSE_PATH}/claimx_task_templates\",\n        \"z_order_columns\": [\"task_id\"],\n        \"description\": \"ClaimX task templates\"\n    },\n    \"claimx_external_links\": {\n        \"path\": f\"{CLAIMX_LAKEHOUSE_PATH}/claimx_external_links\",\n        \"z_order_columns\": [\"project_id\", \"assignment_id\"],\n        \"description\": \"ClaimX external link entities\"\n    },\n    \"claimx_video_collab\": {\n        \"path\": f\"{CLAIMX_LAKEHOUSE_PATH}/claimx_video_collab\",\n        \"z_order_columns\": [\"video_collaboration_id\"],\n        \"description\": \"ClaimX video collaboration sessions\"\n    }\n}\n\n# Combined for iteration\nALL_TABLES = {**XACT_TABLES, **CLAIMX_TABLES}\n\nprint(f\"Configured {len(XACT_TABLES)} XACT tables and {len(CLAIMX_TABLES)} ClaimX tables\")\nprint(f\"Dry run mode: {DRY_RUN}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Any\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, lit, when, sum as spark_sum, max as spark_max, min as spark_min\n",
    "\n",
    "# Get or create Spark session (Fabric provides this automatically)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Enable Delta Lake optimizations\n",
    "spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"false\")  # We'll run manually\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Maintenance run started at: {datetime.now().isoformat()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MaintenanceResult:\n",
    "    \"\"\"Result of a maintenance operation.\"\"\"\n",
    "    table_name: str\n",
    "    operation: str\n",
    "    success: bool\n",
    "    duration_seconds: float = 0.0\n",
    "    rows_affected: int = 0\n",
    "    files_before: int = 0\n",
    "    files_after: int = 0\n",
    "    size_before_mb: float = 0.0\n",
    "    size_after_mb: float = 0.0\n",
    "    error_message: Optional[str] = None\n",
    "    details: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"table_name\": self.table_name,\n",
    "            \"operation\": self.operation,\n",
    "            \"success\": self.success,\n",
    "            \"duration_seconds\": round(self.duration_seconds, 2),\n",
    "            \"rows_affected\": self.rows_affected,\n",
    "            \"files_before\": self.files_before,\n",
    "            \"files_after\": self.files_after,\n",
    "            \"size_before_mb\": round(self.size_before_mb, 2),\n",
    "            \"size_after_mb\": round(self.size_after_mb, 2),\n",
    "            \"error_message\": self.error_message,\n",
    "            \"details\": self.details\n",
    "        }\n",
    "\n",
    "# Store all results for final report\n",
    "maintenance_results: List[MaintenanceResult] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_exists(table_path: str) -> bool:\n",
    "    \"\"\"Check if a Delta table exists at the given path.\"\"\"\n",
    "    try:\n",
    "        DeltaTable.forPath(spark, table_path)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_table_stats(table_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Get statistics for a Delta table.\"\"\"\n",
    "    try:\n",
    "        dt = DeltaTable.forPath(spark, table_path)\n",
    "        detail = dt.detail().collect()[0]\n",
    "        \n",
    "        # Get row count\n",
    "        row_count = spark.read.format(\"delta\").load(table_path).count()\n",
    "        \n",
    "        return {\n",
    "            \"exists\": True,\n",
    "            \"row_count\": row_count,\n",
    "            \"num_files\": detail.numFiles,\n",
    "            \"size_bytes\": detail.sizeInBytes,\n",
    "            \"size_mb\": round(detail.sizeInBytes / (1024 * 1024), 2),\n",
    "            \"created_at\": str(detail.createdAt) if detail.createdAt else None,\n",
    "            \"last_modified\": str(detail.lastModified) if detail.lastModified else None,\n",
    "            \"partitioning\": detail.partitionColumns if detail.partitionColumns else []\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"exists\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "\n",
    "def should_optimize(table_path: str) -> tuple[bool, str]:\n",
    "    \"\"\"Determine if a table needs optimization based on file metrics.\"\"\"\n",
    "    stats = get_table_stats(table_path)\n",
    "    \n",
    "    if not stats.get(\"exists\"):\n",
    "        return False, \"Table does not exist\"\n",
    "    \n",
    "    num_files = stats.get(\"num_files\", 0)\n",
    "    size_mb = stats.get(\"size_mb\", 0)\n",
    "    \n",
    "    if num_files == 0:\n",
    "        return False, \"Table is empty\"\n",
    "    \n",
    "    avg_file_size_mb = size_mb / num_files if num_files > 0 else 0\n",
    "    \n",
    "    # Count small files (estimate based on average)\n",
    "    if avg_file_size_mb < MIN_FILE_SIZE_MB and num_files > SMALL_FILE_THRESHOLD:\n",
    "        return True, f\"Average file size ({avg_file_size_mb:.1f}MB) below threshold ({MIN_FILE_SIZE_MB}MB) with {num_files} files\"\n",
    "    \n",
    "    if num_files > 100 and avg_file_size_mb < TARGET_FILE_SIZE_MB / 2:\n",
    "        return True, f\"Many files ({num_files}) with small average size ({avg_file_size_mb:.1f}MB)\"\n",
    "    \n",
    "    return False, f\"Table is healthy ({num_files} files, {avg_file_size_mb:.1f}MB avg)\"\n",
    "\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Retry Queue Cleanup\n",
    "\n",
    "Remove expired records from retry queues to prevent unbounded growth.\n",
    "\n",
    "Records are deleted when:\n",
    "- `created_at` is older than retention period\n",
    "- For event_log: only cleanup completed/permanent_failed records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_expired_records(\n",
    "    table_name: str,\n",
    "    table_path: str,\n",
    "    retention_days: int,\n",
    "    additional_filter: Optional[str] = None,\n",
    "    dry_run: bool = False\n",
    ") -> MaintenanceResult:\n",
    "    \"\"\"\n",
    "    Delete records older than retention period from a table.\n",
    "    \n",
    "    Args:\n",
    "        table_name: Name of the table for logging\n",
    "        table_path: Full path to the Delta table\n",
    "        retention_days: Number of days to retain\n",
    "        additional_filter: Optional additional SQL filter condition\n",
    "        dry_run: If True, only count records without deleting\n",
    "    \n",
    "    Returns:\n",
    "        MaintenanceResult with operation details\n",
    "    \"\"\"\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        if not table_exists(table_path):\n",
    "            return MaintenanceResult(\n",
    "                table_name=table_name,\n",
    "                operation=\"cleanup\",\n",
    "                success=True,\n",
    "                details={\"message\": \"Table does not exist, skipping\"}\n",
    "            )\n",
    "        \n",
    "        dt = DeltaTable.forPath(spark, table_path)\n",
    "        cutoff_date = datetime.now() - timedelta(days=retention_days)\n",
    "        cutoff_iso = cutoff_date.isoformat()\n",
    "        \n",
    "        # Build delete condition\n",
    "        delete_condition = f\"created_at < '{cutoff_iso}'\"\n",
    "        if additional_filter:\n",
    "            delete_condition = f\"({delete_condition}) AND ({additional_filter})\"\n",
    "        \n",
    "        # Count records to delete\n",
    "        df = spark.read.format(\"delta\").load(table_path)\n",
    "        count_before = df.count()\n",
    "        records_to_delete = df.filter(delete_condition).count()\n",
    "        \n",
    "        if dry_run:\n",
    "            duration = (datetime.now() - start_time).total_seconds()\n",
    "            return MaintenanceResult(\n",
    "                table_name=table_name,\n",
    "                operation=\"cleanup_dry_run\",\n",
    "                success=True,\n",
    "                duration_seconds=duration,\n",
    "                rows_affected=records_to_delete,\n",
    "                details={\n",
    "                    \"cutoff_date\": cutoff_iso,\n",
    "                    \"retention_days\": retention_days,\n",
    "                    \"total_rows\": count_before,\n",
    "                    \"would_delete\": records_to_delete,\n",
    "                    \"filter\": delete_condition\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        # Execute delete\n",
    "        if records_to_delete > 0:\n",
    "            dt.delete(delete_condition)\n",
    "        \n",
    "        count_after = spark.read.format(\"delta\").load(table_path).count()\n",
    "        duration = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        return MaintenanceResult(\n",
    "            table_name=table_name,\n",
    "            operation=\"cleanup\",\n",
    "            success=True,\n",
    "            duration_seconds=duration,\n",
    "            rows_affected=count_before - count_after,\n",
    "            details={\n",
    "                \"cutoff_date\": cutoff_iso,\n",
    "                \"retention_days\": retention_days,\n",
    "                \"rows_before\": count_before,\n",
    "                \"rows_after\": count_after,\n",
    "                \"filter\": delete_condition\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        duration = (datetime.now() - start_time).total_seconds()\n",
    "        return MaintenanceResult(\n",
    "            table_name=table_name,\n",
    "            operation=\"cleanup\",\n",
    "            success=False,\n",
    "            duration_seconds=duration,\n",
    "            error_message=str(e)\n",
    "        )\n",
    "\n",
    "\n",
    "print(\"Cleanup function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute cleanup on tables marked for cleanup\n",
    "print(\"=\"*60)\n",
    "print(\"RETRY QUEUE CLEANUP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cleanup_tables = [\n",
    "    (name, config) for name, config in ALL_TABLES.items() \n",
    "    if config.get(\"cleanup\", False)\n",
    "]\n",
    "\n",
    "print(f\"\\nTables scheduled for cleanup: {len(cleanup_tables)}\")\n",
    "for name, config in cleanup_tables:\n",
    "    print(f\"  - {name}: {config.get('retention_days', RETRY_RETENTION_DAYS)} days retention\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "\n",
    "for table_name, config in cleanup_tables:\n",
    "    print(f\"\\nProcessing: {table_name}\")\n",
    "    \n",
    "    result = cleanup_expired_records(\n",
    "        table_name=table_name,\n",
    "        table_path=config[\"path\"],\n",
    "        retention_days=config.get(\"retention_days\", RETRY_RETENTION_DAYS),\n",
    "        additional_filter=config.get(\"cleanup_filter\"),\n",
    "        dry_run=DRY_RUN\n",
    "    )\n",
    "    \n",
    "    maintenance_results.append(result)\n",
    "    \n",
    "    if result.success:\n",
    "        if result.rows_affected > 0:\n",
    "            print(f\"  {'Would delete' if DRY_RUN else 'Deleted'}: {result.rows_affected:,} records\")\n",
    "        else:\n",
    "            print(f\"  No records to clean up\")\n",
    "        print(f\"  Duration: {result.duration_seconds:.2f}s\")\n",
    "    else:\n",
    "        print(f\"  ERROR: {result.error_message}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Cleanup phase complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Delta Table Optimization\n",
    "\n",
    "Compact small files into larger, more efficient files using Delta Lake OPTIMIZE.\n",
    "\n",
    "Benefits:\n",
    "- Improved query performance (fewer files to scan)\n",
    "- Reduced metadata overhead\n",
    "- Z-ordering for data co-location on common query columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_table(\n",
    "    table_name: str,\n",
    "    table_path: str,\n",
    "    z_order_columns: Optional[List[str]] = None,\n",
    "    dry_run: bool = False\n",
    ") -> MaintenanceResult:\n",
    "    \"\"\"\n",
    "    Optimize a Delta table through file compaction and optional Z-ordering.\n",
    "    \n",
    "    Args:\n",
    "        table_name: Name of the table for logging\n",
    "        table_path: Full path to the Delta table\n",
    "        z_order_columns: Columns to Z-order by for query optimization\n",
    "        dry_run: If True, only analyze without optimizing\n",
    "    \n",
    "    Returns:\n",
    "        MaintenanceResult with operation details\n",
    "    \"\"\"\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        if not table_exists(table_path):\n",
    "            return MaintenanceResult(\n",
    "                table_name=table_name,\n",
    "                operation=\"optimize\",\n",
    "                success=True,\n",
    "                details={\"message\": \"Table does not exist, skipping\"}\n",
    "            )\n",
    "        \n",
    "        # Check if optimization is needed\n",
    "        needs_opt, reason = should_optimize(table_path)\n",
    "        stats_before = get_table_stats(table_path)\n",
    "        \n",
    "        if not needs_opt:\n",
    "            duration = (datetime.now() - start_time).total_seconds()\n",
    "            return MaintenanceResult(\n",
    "                table_name=table_name,\n",
    "                operation=\"optimize_skipped\",\n",
    "                success=True,\n",
    "                duration_seconds=duration,\n",
    "                files_before=stats_before.get(\"num_files\", 0),\n",
    "                size_before_mb=stats_before.get(\"size_mb\", 0),\n",
    "                details={\"reason\": reason}\n",
    "            )\n",
    "        \n",
    "        if dry_run:\n",
    "            duration = (datetime.now() - start_time).total_seconds()\n",
    "            return MaintenanceResult(\n",
    "                table_name=table_name,\n",
    "                operation=\"optimize_dry_run\",\n",
    "                success=True,\n",
    "                duration_seconds=duration,\n",
    "                files_before=stats_before.get(\"num_files\", 0),\n",
    "                size_before_mb=stats_before.get(\"size_mb\", 0),\n",
    "                details={\n",
    "                    \"would_optimize\": True,\n",
    "                    \"reason\": reason,\n",
    "                    \"z_order_columns\": z_order_columns\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        # Execute OPTIMIZE\n",
    "        dt = DeltaTable.forPath(spark, table_path)\n",
    "        \n",
    "        if z_order_columns:\n",
    "            # Validate Z-order columns exist\n",
    "            df_schema = spark.read.format(\"delta\").load(table_path).columns\n",
    "            valid_columns = [c for c in z_order_columns if c in df_schema]\n",
    "            \n",
    "            if valid_columns:\n",
    "                dt.optimize().executeZOrderBy(valid_columns)\n",
    "            else:\n",
    "                dt.optimize().executeCompaction()\n",
    "        else:\n",
    "            dt.optimize().executeCompaction()\n",
    "        \n",
    "        # Get stats after optimization\n",
    "        stats_after = get_table_stats(table_path)\n",
    "        duration = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        return MaintenanceResult(\n",
    "            table_name=table_name,\n",
    "            operation=\"optimize\",\n",
    "            success=True,\n",
    "            duration_seconds=duration,\n",
    "            files_before=stats_before.get(\"num_files\", 0),\n",
    "            files_after=stats_after.get(\"num_files\", 0),\n",
    "            size_before_mb=stats_before.get(\"size_mb\", 0),\n",
    "            size_after_mb=stats_after.get(\"size_mb\", 0),\n",
    "            details={\n",
    "                \"z_order_columns\": z_order_columns,\n",
    "                \"files_removed\": stats_before.get(\"num_files\", 0) - stats_after.get(\"num_files\", 0)\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        duration = (datetime.now() - start_time).total_seconds()\n",
    "        return MaintenanceResult(\n",
    "            table_name=table_name,\n",
    "            operation=\"optimize\",\n",
    "            success=False,\n",
    "            duration_seconds=duration,\n",
    "            error_message=str(e)\n",
    "        )\n",
    "\n",
    "\n",
    "print(\"Optimize function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute optimization on all tables\n",
    "print(\"=\"*60)\n",
    "print(\"DELTA TABLE OPTIMIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nTables to process: {len(ALL_TABLES)}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for table_name, config in ALL_TABLES.items():\n",
    "    print(f\"\\nProcessing: {table_name}\")\n",
    "    print(f\"  Description: {config.get('description', 'N/A')}\")\n",
    "    \n",
    "    result = optimize_table(\n",
    "        table_name=table_name,\n",
    "        table_path=config[\"path\"],\n",
    "        z_order_columns=config.get(\"z_order_columns\"),\n",
    "        dry_run=DRY_RUN\n",
    "    )\n",
    "    \n",
    "    maintenance_results.append(result)\n",
    "    \n",
    "    if result.success:\n",
    "        if \"skipped\" in result.operation:\n",
    "            print(f\"  Skipped: {result.details.get('reason', 'N/A')}\")\n",
    "        elif \"dry_run\" in result.operation:\n",
    "            print(f\"  Would optimize: {result.files_before} files ({result.size_before_mb:.1f} MB)\")\n",
    "            print(f\"  Reason: {result.details.get('reason', 'N/A')}\")\n",
    "        else:\n",
    "            files_removed = result.files_before - result.files_after\n",
    "            print(f\"  Files: {result.files_before} -> {result.files_after} ({files_removed} compacted)\")\n",
    "            print(f\"  Size: {result.size_before_mb:.1f} MB -> {result.size_after_mb:.1f} MB\")\n",
    "        print(f\"  Duration: {result.duration_seconds:.2f}s\")\n",
    "    else:\n",
    "        print(f\"  ERROR: {result.error_message}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Optimization phase complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. VACUUM Operations\n",
    "\n",
    "Remove old file versions from Delta tables to reclaim storage.\n",
    "\n",
    "**Important**: VACUUM removes old files that are no longer referenced by the current version of the table. The retention period should be set to allow for time travel queries within your SLA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vacuum_table(\n",
    "    table_name: str,\n",
    "    table_path: str,\n",
    "    retention_hours: int = 168,  # 7 days default\n",
    "    dry_run: bool = False\n",
    ") -> MaintenanceResult:\n",
    "    \"\"\"\n",
    "    Run VACUUM on a Delta table to remove old file versions.\n",
    "    \n",
    "    Args:\n",
    "        table_name: Name of the table for logging\n",
    "        table_path: Full path to the Delta table\n",
    "        retention_hours: Hours to retain old versions (min 168 = 7 days)\n",
    "        dry_run: If True, only report without vacuuming\n",
    "    \n",
    "    Returns:\n",
    "        MaintenanceResult with operation details\n",
    "    \"\"\"\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        if not table_exists(table_path):\n",
    "            return MaintenanceResult(\n",
    "                table_name=table_name,\n",
    "                operation=\"vacuum\",\n",
    "                success=True,\n",
    "                details={\"message\": \"Table does not exist, skipping\"}\n",
    "            )\n",
    "        \n",
    "        dt = DeltaTable.forPath(spark, table_path)\n",
    "        stats_before = get_table_stats(table_path)\n",
    "        \n",
    "        if dry_run:\n",
    "            # Dry run - just report what would be cleaned\n",
    "            files_to_delete = dt.vacuum(retention_hours, dry_run=True)\n",
    "            \n",
    "            # Convert to count if it's a list or DataFrame\n",
    "            if hasattr(files_to_delete, 'count'):\n",
    "                file_count = files_to_delete.count()\n",
    "            elif isinstance(files_to_delete, list):\n",
    "                file_count = len(files_to_delete)\n",
    "            else:\n",
    "                file_count = 0\n",
    "            \n",
    "            duration = (datetime.now() - start_time).total_seconds()\n",
    "            return MaintenanceResult(\n",
    "                table_name=table_name,\n",
    "                operation=\"vacuum_dry_run\",\n",
    "                success=True,\n",
    "                duration_seconds=duration,\n",
    "                size_before_mb=stats_before.get(\"size_mb\", 0),\n",
    "                details={\n",
    "                    \"retention_hours\": retention_hours,\n",
    "                    \"files_to_delete\": file_count\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        # Execute VACUUM\n",
    "        dt.vacuum(retention_hours)\n",
    "        \n",
    "        stats_after = get_table_stats(table_path)\n",
    "        duration = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        return MaintenanceResult(\n",
    "            table_name=table_name,\n",
    "            operation=\"vacuum\",\n",
    "            success=True,\n",
    "            duration_seconds=duration,\n",
    "            size_before_mb=stats_before.get(\"size_mb\", 0),\n",
    "            size_after_mb=stats_after.get(\"size_mb\", 0),\n",
    "            details={\n",
    "                \"retention_hours\": retention_hours,\n",
    "                \"space_reclaimed_mb\": round(stats_before.get(\"size_mb\", 0) - stats_after.get(\"size_mb\", 0), 2)\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        duration = (datetime.now() - start_time).total_seconds()\n",
    "        return MaintenanceResult(\n",
    "            table_name=table_name,\n",
    "            operation=\"vacuum\",\n",
    "            success=False,\n",
    "            duration_seconds=duration,\n",
    "            error_message=str(e)\n",
    "        )\n",
    "\n",
    "\n",
    "print(\"Vacuum function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute VACUUM on all tables\n",
    "print(\"=\"*60)\n",
    "print(\"VACUUM OPERATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nRetention period: {VACUUM_RETENTION_HOURS} hours ({VACUUM_RETENTION_HOURS/24:.1f} days)\")\n",
    "print(f\"Tables to process: {len(ALL_TABLES)}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for table_name, config in ALL_TABLES.items():\n",
    "    print(f\"\\nProcessing: {table_name}\")\n",
    "    \n",
    "    result = vacuum_table(\n",
    "        table_name=table_name,\n",
    "        table_path=config[\"path\"],\n",
    "        retention_hours=VACUUM_RETENTION_HOURS,\n",
    "        dry_run=DRY_RUN\n",
    "    )\n",
    "    \n",
    "    maintenance_results.append(result)\n",
    "    \n",
    "    if result.success:\n",
    "        if \"dry_run\" in result.operation:\n",
    "            print(f\"  Files to delete: {result.details.get('files_to_delete', 0)}\")\n",
    "        else:\n",
    "            space_reclaimed = result.details.get(\"space_reclaimed_mb\", 0)\n",
    "            print(f\"  Space reclaimed: {space_reclaimed:.1f} MB\")\n",
    "        print(f\"  Duration: {result.duration_seconds:.2f}s\")\n",
    "    else:\n",
    "        print(f\"  ERROR: {result.error_message}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Vacuum phase complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Health Report\n",
    "\n",
    "Generate a comprehensive health report for all tables and retry queues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retry_queue_health(table_name: str, table_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get health metrics for a retry queue table.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with retry queue statistics\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not table_exists(table_path):\n",
    "            return {\"exists\": False}\n",
    "        \n",
    "        df = spark.read.format(\"delta\").load(table_path)\n",
    "        \n",
    "        # Aggregate statistics\n",
    "        stats = df.agg(\n",
    "            count(\"*\").alias(\"total_rows\"),\n",
    "            spark_sum(when(col(\"status\") == \"failed\", 1).otherwise(0)).alias(\"status_failed\"),\n",
    "            spark_sum(when(col(\"status\") == \"failed_permanent\", 1).otherwise(0)).alias(\"status_permanent\"),\n",
    "            spark_sum(when(col(\"retry_count\") == 0, 1).otherwise(0)).alias(\"retry_0\"),\n",
    "            spark_sum(when(col(\"retry_count\") == 1, 1).otherwise(0)).alias(\"retry_1\"),\n",
    "            spark_sum(when(col(\"retry_count\") == 2, 1).otherwise(0)).alias(\"retry_2\"),\n",
    "            spark_sum(when(col(\"retry_count\") >= 3, 1).otherwise(0)).alias(\"retry_3_plus\"),\n",
    "            spark_min(\"created_at\").alias(\"oldest_record\"),\n",
    "            spark_max(\"created_at\").alias(\"newest_record\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        return {\n",
    "            \"exists\": True,\n",
    "            \"total_rows\": stats.total_rows,\n",
    "            \"status_failed\": stats.status_failed or 0,\n",
    "            \"status_permanent\": stats.status_permanent or 0,\n",
    "            \"retry_distribution\": {\n",
    "                \"retry_0\": stats.retry_0 or 0,\n",
    "                \"retry_1\": stats.retry_1 or 0,\n",
    "                \"retry_2\": stats.retry_2 or 0,\n",
    "                \"retry_3_plus\": stats.retry_3_plus or 0\n",
    "            },\n",
    "            \"oldest_record\": str(stats.oldest_record) if stats.oldest_record else None,\n",
    "            \"newest_record\": str(stats.newest_record) if stats.newest_record else None\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"exists\": False, \"error\": str(e)}\n",
    "\n",
    "\n",
    "print(\"Health report functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive health report\n",
    "print(\"=\"*60)\n",
    "print(\"TABLE HEALTH REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nGenerated at: {datetime.now().isoformat()}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "health_report = {}\n",
    "\n",
    "for table_name, config in ALL_TABLES.items():\n",
    "    print(f\"\\n{table_name}\")\n",
    "    print(f\"  Path: {config['path']}\")\n",
    "    \n",
    "    stats = get_table_stats(config[\"path\"])\n",
    "    \n",
    "    if stats.get(\"exists\"):\n",
    "        print(f\"  Rows: {stats.get('row_count', 0):,}\")\n",
    "        print(f\"  Files: {stats.get('num_files', 0):,}\")\n",
    "        print(f\"  Size: {stats.get('size_mb', 0):.1f} MB\")\n",
    "        \n",
    "        if stats.get('num_files', 0) > 0:\n",
    "            avg_size = stats.get('size_mb', 0) / stats.get('num_files', 1)\n",
    "            print(f\"  Avg file size: {avg_size:.1f} MB\")\n",
    "        \n",
    "        # Additional stats for retry tables\n",
    "        if \"retry\" in table_name.lower():\n",
    "            retry_stats = get_retry_queue_health(table_name, config[\"path\"])\n",
    "            if retry_stats.get(\"exists\"):\n",
    "                print(f\"  Status - Failed: {retry_stats.get('status_failed', 0):,}, Permanent: {retry_stats.get('status_permanent', 0):,}\")\n",
    "                dist = retry_stats.get(\"retry_distribution\", {})\n",
    "                print(f\"  Retry counts - 0: {dist.get('retry_0', 0)}, 1: {dist.get('retry_1', 0)}, 2: {dist.get('retry_2', 0)}, 3+: {dist.get('retry_3_plus', 0)}\")\n",
    "            stats[\"retry_health\"] = retry_stats\n",
    "    else:\n",
    "        print(f\"  Status: Does not exist or empty\")\n",
    "    \n",
    "    health_report[table_name] = stats\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of retry queues\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RETRY QUEUE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "retry_tables = [name for name in ALL_TABLES.keys() if \"retry\" in name.lower()]\n",
    "\n",
    "for table_name in retry_tables:\n",
    "    stats = health_report.get(table_name, {})\n",
    "    retry_health = stats.get(\"retry_health\", {})\n",
    "    \n",
    "    if retry_health.get(\"exists\"):\n",
    "        total = retry_health.get(\"total_rows\", 0)\n",
    "        failed = retry_health.get(\"status_failed\", 0)\n",
    "        permanent = retry_health.get(\"status_permanent\", 0)\n",
    "        \n",
    "        print(f\"\\n{table_name}:\")\n",
    "        print(f\"  Total records: {total:,}\")\n",
    "        print(f\"  Active retries (failed): {failed:,}\")\n",
    "        print(f\"  Permanent failures: {permanent:,}\")\n",
    "        \n",
    "        # Calculate retry rate\n",
    "        dist = retry_health.get(\"retry_distribution\", {})\n",
    "        retry_0 = dist.get(\"retry_0\", 0)\n",
    "        if total > 0:\n",
    "            first_attempt_rate = (retry_0 / total) * 100\n",
    "            print(f\"  First-attempt failures: {first_attempt_rate:.1f}%\")\n",
    "        \n",
    "        # Age analysis\n",
    "        oldest = retry_health.get(\"oldest_record\")\n",
    "        if oldest and oldest != \"None\":\n",
    "            print(f\"  Oldest record: {oldest}\")\n",
    "    else:\n",
    "        print(f\"\\n{table_name}: No data or table does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Maintenance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MAINTENANCE RUN SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nCompleted at: {datetime.now().isoformat()}\")\n",
    "print(f\"Dry run mode: {DRY_RUN}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Group results by operation type\n",
    "operations = {}\n",
    "for result in maintenance_results:\n",
    "    op = result.operation.replace(\"_dry_run\", \"\").replace(\"_skipped\", \"\")\n",
    "    if op not in operations:\n",
    "        operations[op] = {\"success\": 0, \"failed\": 0, \"total_duration\": 0, \"rows_affected\": 0}\n",
    "    \n",
    "    if result.success:\n",
    "        operations[op][\"success\"] += 1\n",
    "    else:\n",
    "        operations[op][\"failed\"] += 1\n",
    "    \n",
    "    operations[op][\"total_duration\"] += result.duration_seconds\n",
    "    operations[op][\"rows_affected\"] += result.rows_affected\n",
    "\n",
    "print(\"\\nOperation Summary:\")\n",
    "for op, stats in operations.items():\n",
    "    print(f\"  {op}:\")\n",
    "    print(f\"    Success: {stats['success']}, Failed: {stats['failed']}\")\n",
    "    print(f\"    Duration: {stats['total_duration']:.1f}s\")\n",
    "    if stats['rows_affected'] > 0:\n",
    "        print(f\"    Rows affected: {stats['rows_affected']:,}\")\n",
    "\n",
    "# List any failures\n",
    "failures = [r for r in maintenance_results if not r.success]\n",
    "if failures:\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"FAILURES:\")\n",
    "    for f in failures:\n",
    "        print(f\"  {f.table_name} ({f.operation}): {f.error_message}\")\n",
    "else:\n",
    "    print(\"\\nAll operations completed successfully!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export detailed results as JSON for logging/monitoring\n",
    "results_json = {\n",
    "    \"run_timestamp\": datetime.now().isoformat(),\n",
    "    \"dry_run\": DRY_RUN,\n",
    "    \"configuration\": {\n",
    "        \"retry_retention_days\": RETRY_RETENTION_DAYS,\n",
    "        \"event_log_retention_days\": EVENT_LOG_RETENTION_DAYS,\n",
    "        \"vacuum_retention_hours\": VACUUM_RETENTION_HOURS,\n",
    "        \"target_file_size_mb\": TARGET_FILE_SIZE_MB\n",
    "    },\n",
    "    \"results\": [r.to_dict() for r in maintenance_results],\n",
    "    \"health_report\": health_report,\n",
    "    \"summary\": operations\n",
    "}\n",
    "\n",
    "print(\"\\nDetailed results (JSON):\")\n",
    "print(json.dumps(results_json, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notes for Scheduling\n",
    "\n",
    "### Recommended Schedule\n",
    "\n",
    "| Operation | Frequency | Recommended Time |\n",
    "|-----------|-----------|------------------|\n",
    "| Cleanup | Daily | 2:00 AM |\n",
    "| Optimize | Daily | After cleanup |\n",
    "| Vacuum | Weekly | Sunday 3:00 AM |\n",
    "| Health Report | Daily | After all operations |\n",
    "\n",
    "### Fabric Scheduling\n",
    "\n",
    "1. Go to your Fabric workspace\n",
    "2. Select this notebook\n",
    "3. Click **Schedule** in the toolbar\n",
    "4. Configure the schedule (daily at 2:00 AM recommended)\n",
    "5. Enable notifications for failures\n",
    "\n",
    "### Monitoring\n",
    "\n",
    "- Check the health report section for growing retry queues\n",
    "- Monitor for permanent failures in retry tables\n",
    "- Alert if optimization operations fail consistently\n",
    "- Track storage growth over time\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "If VACUUM fails with retention errors:\n",
    "- Ensure `VACUUM_RETENTION_HOURS` >= 168 (7 days)\n",
    "- Check for concurrent operations on the table\n",
    "\n",
    "If OPTIMIZE is slow:\n",
    "- Consider partitioning high-volume tables\n",
    "- Run during off-peak hours\n",
    "- Increase Spark executor memory if available"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}