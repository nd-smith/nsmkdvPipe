# Kafka Pipeline Environment Configuration
# Copy this file to .env and fill in your values

# =============================================================================
# AZURE EVENT HUB (Source of Truth)
# =============================================================================
# Required for production mode. Not needed if running with --dev flag.

# Event Hub namespace with Kafka endpoint (e.g., namespace.servicebus.windows.net:9093)
EVENTHUB_BOOTSTRAP_SERVERS=your-namespace.servicebus.windows.net:9093

# Full Event Hub connection string from Azure Portal
# Format: Endpoint=sb://<namespace>.servicebus.windows.net/;SharedAccessKeyName=<name>;SharedAccessKey=<key>;EntityPath=<eventhub>
EVENTHUB_CONNECTION_STRING=Endpoint=sb://your-namespace.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=your-key-here

# Event Hub topic name (default: xact.events.raw)
EVENTHUB_EVENTS_TOPIC=xact.events.raw

# Consumer group for reading events (default: xact-event-ingester)
EVENTHUB_CONSUMER_GROUP=xact-event-ingester

# Where to start reading: earliest, latest (default: earliest)
EVENTHUB_AUTO_OFFSET_RESET=earliest


# =============================================================================
# LOCAL KAFKA (Internal Pipeline Communication)
# =============================================================================
# Used for communication between workers (downloads.pending, downloads.results, etc.)

# Local Kafka broker address (default: localhost:9092)
LOCAL_KAFKA_BOOTSTRAP_SERVERS=localhost:9092

# Security protocol: PLAINTEXT, SASL_SSL, SASL_PLAINTEXT (default: PLAINTEXT)
LOCAL_KAFKA_SECURITY_PROTOCOL=PLAINTEXT

# Topic names (all have sensible defaults)
KAFKA_DOWNLOADS_PENDING_TOPIC=xact.downloads.pending
KAFKA_DOWNLOADS_CACHED_TOPIC=xact.downloads.cached
KAFKA_DOWNLOADS_RESULTS_TOPIC=xact.downloads.results
KAFKA_DLQ_TOPIC=xact.downloads.dlq

# Consumer group prefix (default: xact)
KAFKA_CONSUMER_GROUP_PREFIX=xact


# =============================================================================
# RETRY CONFIGURATION
# =============================================================================

# Comma-separated retry delays in seconds (default: 300,600,1200,2400)
# These create topics: xact.downloads.pending.retry.5m, retry.10m, retry.20m, retry.40m
RETRY_DELAYS=300,600,1200,2400

# Maximum retry attempts before sending to DLQ (default: 4)
MAX_RETRIES=4


# =============================================================================
# AZURE STORAGE (OneLake / ADLS)
# =============================================================================

# OneLake base path for uploading downloaded files
# Format: abfss://<workspace>@onelake.dfs.fabric.microsoft.com/<lakehouse>/Files
ONELAKE_BASE_PATH=abfss://your-workspace@onelake.dfs.fabric.microsoft.com/your-lakehouse/Files


# =============================================================================
# DELTA LAKE
# =============================================================================

# Enable/disable Delta Lake writes (default: true)
ENABLE_DELTA_WRITES=true

# Path to Delta Lake events table (for deduplication and analytics)
DELTA_EVENTS_TABLE_PATH=abfss://your-workspace@onelake.dfs.fabric.microsoft.com/your-lakehouse/Tables/xact_events

# Path to Delta Lake inventory table (for tracking downloaded attachments)
DELTA_INVENTORY_TABLE_PATH=abfss://your-workspace@onelake.dfs.fabric.microsoft.com/your-lakehouse/Tables/xact_attachments

# Path to Delta Lake failed attachments table (for tracking permanent failures)
# Optional: If not set, permanent failures are only tracked in DLQ
DELTA_FAILED_TABLE_PATH=abfss://your-workspace@onelake.dfs.fabric.microsoft.com/your-lakehouse/Tables/xact_attachments_failed


# =============================================================================
# AZURE AUTHENTICATION
# =============================================================================
# For OAuth/SPN authentication to Azure services

# Azure tenant ID
AZURE_TENANT_ID=your-tenant-id

# Service Principal credentials (for production)
AZURE_CLIENT_ID=your-client-id
AZURE_CLIENT_SECRET=your-client-secret

# Or use Azure CLI auth for local development:
# az login


# =============================================================================
# WORKER CONFIGURATION
# =============================================================================

# Download worker concurrency (default: 10, max: 50)
DOWNLOAD_CONCURRENCY=10
DOWNLOAD_BATCH_SIZE=20

# Upload worker concurrency (default: 10, max: 50)
UPLOAD_CONCURRENCY=10
UPLOAD_BATCH_SIZE=20

# Local cache directory for downloaded files awaiting upload
# Files are cached here between download and upload workers
CACHE_DIR=/tmp/kafka_pipeline_cache


# =============================================================================
# OBSERVABILITY
# =============================================================================

# Metrics port for Prometheus scraping (default: 8000)
METRICS_PORT=8000

# Log level: DEBUG, INFO, WARNING, ERROR (default: INFO)
LOG_LEVEL=INFO


# =============================================================================
# LOCAL DEVELOPMENT
# =============================================================================
# Quick setup for local Kafka (using docker-compose or similar)

# To run with local Kafka only (no Event Hub):
#   python -m kafka_pipeline --dev

# To start local Kafka with Docker:
#   docker run -d --name kafka -p 9092:9092 \
#     -e KAFKA_CFG_NODE_ID=0 \
#     -e KAFKA_CFG_PROCESS_ROLES=controller,broker \
#     -e KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093 \
#     -e KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT \
#     -e KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@localhost:9093 \
#     -e KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER \
#     bitnami/kafka:latest

# Create topics:
#   docker exec kafka kafka-topics.sh --create --topic xact.downloads.pending --bootstrap-server localhost:9092
#   docker exec kafka kafka-topics.sh --create --topic xact.downloads.cached --bootstrap-server localhost:9092
#   docker exec kafka kafka-topics.sh --create --topic xact.downloads.results --bootstrap-server localhost:9092
#   docker exec kafka kafka-topics.sh --create --topic xact.downloads.dlq --bootstrap-server localhost:9092
